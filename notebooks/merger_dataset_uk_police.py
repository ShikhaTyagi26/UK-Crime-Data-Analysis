# -*- coding: utf-8 -*-
"""Merger Dataset UK Police.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C_DbzbfzmIcHWIzlCxr1cSnCTxZU4kHW

#**UK Police Crime Dataset Jan 2025 - Sept 2025 : Data merging (step 1)**

---

**Name :** Shikha Tyagi

***MSc Business Analytics***

**Student ID:** 20052009

**Dissertation Topic:** Crime Hotspot Detection Using Geospatial
Visualisation and Unsupervised Clustering : A*n Unsupervised Learning Approach on UK Open Crime Data*


**Raw Data:** Uploaded on Drive

**Street Crime Data Jan -Sept 2025:**

https://drive.google.com/drive/folders/19dlHdCZr5T3pbXb-CUJStf2OWQO0IRCR?usp=drive_link

**Outcome Data Jan - Spet 2025:**

https://drive.google.com/drive/folders/17D0yMTcReNL3LARxPsAOThVl75KqYXIr?usp=drive_link

**This Code used for merging the UK Crime Open Dataset from Jan 2025 to Sept 2025**
"""

# Loading important libraries
import pandas as pd
import glob
import os

# ============================
# 1 MERGE STREET DATA
# ============================

street_folder = "/content/drive/MyDrive/Crime Data UK" #connecting to drive where the street crime files are uploaded
street_files = glob.glob(os.path.join(street_folder, "**/*-street.csv"), recursive=True)

street_dfs = []
for f in street_files:  # for loop for reading all the files
    df = pd.read_csv(f)
    df.columns = df.columns.str.lower().str.strip()
    df = df.rename(columns={'crime id': 'crime_id'})
    df['crime_id'] = df['crime_id'].astype(str).str.strip()

    street_dfs.append(df) #appending all the files into a single file

merged_street = pd.concat(street_dfs, ignore_index=True).drop_duplicates() # concatinating the files and dropping the duplicates

print("Street Crime IDs:", merged_street['crime_id'].nunique())


# ============================
# 2 MERGE OUTCOME DATA
# ============================

outcome_folder = "/content/drive/MyDrive/UK Outcome Data"
outcome_files = glob.glob(os.path.join(outcome_folder, "**/*-outcomes.csv"), recursive=True)

outcome_dfs = []
for f in outcome_files:
    df = pd.read_csv(f)
    df.columns = df.columns.str.lower().str.strip()
    df = df.rename(columns={'crime id': 'crime_id'})
    df['crime_id'] = df['crime_id'].astype(str).str.strip()

    # clean outcome column
    df = df.rename(columns={'outcome type': 'outcome_type'})

    outcome_dfs.append(df) #appending all the files into a single file

merged_outcomes = pd.concat(outcome_dfs, ignore_index=True).drop_duplicates() # concatinating the files and dropping the duplicates

print("Outcome Crime IDs:", merged_outcomes['crime_id'].nunique())


# ============================
# 3 CHECK MATCHES BEFORE MERGING
# ============================

street_ids = set(merged_street['crime_id'])
outcome_ids = set(merged_outcomes['crime_id'])

matches = street_ids.intersection(outcome_ids) # matching crime_id's column before merging the two datasets

print("Matching Crime IDs:", len(matches))


# ============================
# 4 FINAL LEFT JOIN
# ============================

merged_final = merged_street.merge(          # merging street file with outcome file using left join
    merged_outcomes[['crime_id', 'outcome_type']], # merging using left join and keeping only two columns crime_id and outcome_type from outcome dataset
    on='crime_id',
    how='left'
)

merged_final.to_csv("/content/Street_to_Outcomes_Merged_final.csv", index=False) # Saving the final merged final into CSV for future analysis

print("FINAL MERGE COMPLETE!")
print("Final Rows:", len(merged_final))
print("Outcome column filled values:", merged_final['outcome_type'].notna().sum()) #

"""#*Street_to_Outcomes_Merged_final. csv dataset is the merged raw dataset which will be used for cleaning and analysis*"""