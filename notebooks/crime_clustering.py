# -*- coding: utf-8 -*-
"""Crime Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KitzfCq63tguuz74UZu72lUZH7J4i0CG

# **UK Police Crime Dataset Jan 2025 - Sept 2025 : Data Clustering and Analysis(step 3)**

---

**Name :** Shikha Tyagi

***MSc Business Analytics***

**Student ID:** 20052009

**Dissertation Topic:** Crime Hotspot Detection Using Geospatial
Visualisation and Unsupervised Clustering : A*n Unsupervised Learning Approach on UK Open Crime Data*

**Dataset link:**

https://drive.google.com/file/d/19jYzIRPTZZuTnKMTHIJFDNHuqv_4JNEu/view?usp=drive_link

***Analysis and Vizualization using the Cleaned crime data by applying clustering techniques***
"""

#importing all the necessary libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
import umap.umap_ as umap
import matplotlib.pyplot as plt

# -------------------------
# Loading cleaned data
# -------------------------
df = pd.read_csv("/content/Cleaned_crime_data.csv")
print("Dataset loaded:", df.shape)

# -------------------------
df = df[['latitude', 'longitude']].dropna() #making sure no null value exists

print("Rows with valid geo-coordinates:", df.shape)

# -------------------------
sample_size = 50000  # taking sample limit to avoid RAM crashing
df_sample = df.sample(sample_size, random_state=42)

print("Sample used for clustering:", df_sample.shape)

# -------------------------
# Preprocessing the sampled data
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
geo_scaled = scaler.fit_transform(df_sample)

print("Scaled geo data shape:", geo_scaled.shape)

# -------------------------
# DBSCAN clustering
from sklearn.cluster import DBSCAN

dbscan = DBSCAN(
    eps=0.3,          # neighbourhood radius
    min_samples=20    # minimum points to form hotspot
)

dbscan_labels = dbscan.fit_predict(geo_scaled)

df_sample['dbscan_cluster'] = dbscan_labels

print("DBSCAN clusters found:", set(dbscan_labels))
print(df_sample['dbscan_cluster'].value_counts().head())

#Interpretation

# -1 → noise (isolated crimes)

# 0,1,2 ..... → spatial hotspots

# -------------------------
# HDBSCAN clustering
import hdbscan

hdb = hdbscan.HDBSCAN(
    min_cluster_size=80,  # minimum cluster size
    min_samples=30,   # minimum points to form hotspot
)

hdb_labels = hdb.fit_predict(geo_scaled)

df_sample['hdbscan_cluster'] = hdb_labels

print("Number of HDBSCAN clusters:",
      len(set(hdb_labels)))

print(df_sample['hdbscan_cluster'].value_counts().head())

# -------------------------

# Vizualization of Clusters

# DBSCAN with noise

import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
plt.scatter(
    df_sample['longitude'],
    df_sample['latitude'],
    c=df_sample['dbscan_cluster'],
    cmap='tab20',
    s=5
)
plt.title("DBSCAN Crime Hotspots (50,000 sample)")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.show()

# HDBSCAN with noise
plt.figure(figsize=(10,6))
plt.scatter(
    df_sample['longitude'],
    df_sample['latitude'],
    c=df_sample['hdbscan_cluster'],
    cmap='tab20',
    s=5
)
plt.title("HDBSCAN Crime Hotspots (50,000 sample)")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.show()

# Interpretation

# Irregular shapes → realistic crime geography

# -------------------------

# Cluster summary

cluster_summary = (
    df_sample[df_sample['hdbscan_cluster'] != -1]
    .groupby('hdbscan_cluster')
    .agg(
        crime_count=('latitude', 'count'),
        mean_latitude=('latitude', 'mean'),
        mean_longitude=('longitude', 'mean')
    )
    .sort_values(by='crime_count', ascending=False)
)
print(cluster_summary.to_string())
print(cluster_summary.to_string())
pd.set_option('display.max_rows', None)

"""***Saving clustering results and summary***"""

df_sample.to_csv(
    "/content/Clustering_Results_50k.csv",
    index=False
)

cluster_summary.to_csv(
    "/content/Cluster_Summary.csv"
)

print("Clustering results saved successfully")

"""***Plotting Clusters Without Noise***"""

# DBSCAN Cluster without noise
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

# Re-create df_sample with dbscan_cluster for plotting

sample_size = 50000
df = pd.read_csv("/content/Cleaned_crime_data.csv")
df = df[['latitude', 'longitude']].dropna()
df_sample = df.sample(sample_size, random_state=42)

scaler = StandardScaler()
geo_scaled = scaler.fit_transform(df_sample)

dbscan = DBSCAN(
    eps=0.3,          # neighbourhood radius
    min_samples=20    # minimum points to form hotspot
)

df_labels= dbscan.fit_predict(geo_scaled)
df_sample['dbscan_cluster'] = df_labels

# Keep only real clusters (remove noise)
plot_df = df_sample[df_sample['dbscan_cluster'] != -1]

plt.figure(figsize=(10,12))
plt.scatter(
    plot_df['longitude'],
    plot_df['latitude'],
    c=plot_df['dbscan_cluster'],
    cmap='tab20',
    s=5,
    alpha=0.7
)

plt.title("DBSCAN Crime Hotspots (50,000 sample) without noise")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.show()

# HDBSCAN Clusters without noise
import pandas as pd
from sklearn.preprocessing import StandardScaler
import hdbscan
import matplotlib.pyplot as plt

# Re-create df_sample with hdbscan_cluster for plotting

sample_size = 50000
df = pd.read_csv("/content/Cleaned_crime_data.csv")
df = df[['latitude', 'longitude']].dropna()
df_sample = df.sample(sample_size, random_state=42)

scaler = StandardScaler()
geo_scaled = scaler.fit_transform(df_sample)

hdb = hdbscan.HDBSCAN(
    min_cluster_size=80,
    min_samples=30,
)
hdb_labels = hdb.fit_predict(geo_scaled)
df_sample['hdbscan_cluster'] = hdb_labels


# Keep only real clusters (remove noise)
plot_df = df_sample[df_sample['hdbscan_cluster'] != -1]

plt.figure(figsize=(10,12))
plt.scatter(
    plot_df['longitude'],
    plot_df['latitude'],
    c=plot_df['hdbscan_cluster'],
    cmap='tab20',
    s=5,
    alpha=0.7
)

plt.title("HDBSCAN Crime Hotspots (50,000 sample)")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.show()

"""***Vizualising HDBSCAN clusters on UK map***"""

import folium

# Center on UK
uk_map = folium.Map(location=[52.5, -1.5], zoom_start=6, tiles="cartodbpositron")

# Colour palette
colors = [
    'red','blue','green','purple','orange','darkred','lightred',
    'beige','darkblue','darkgreen','cadetblue','darkpurple',
    'pink','lightblue','lightgreen','gray','black','lightgray'
]

for _, row in plot_df.iterrows():
    folium.CircleMarker(
        location=[row['latitude'], row['longitude']],
        radius=2,
        color=colors[int(row['hdbscan_cluster']) % len(colors)],
        fill=True,
        fill_opacity=0.6
    ).add_to(uk_map)

uk_map.save("HDBSCAN_hotspot_map without noise.html")

"""***Analysis on Cluster V/S Crime Type Composition***"""

import pandas as pd

# Reload the original data to ensure 'crime type' is available
df = pd.read_csv("/content/Cleaned_crime_data.csv")

# Keep only required columns
df = df[['latitude', 'longitude', 'crime type']].dropna()

# Random sample
df_sample = df.sample(50000, random_state=42)

print("Sample shape:", df_sample.shape)

from sklearn.preprocessing import StandardScaler

# Scale geospatial features
scaler = StandardScaler()
coords_scaled = scaler.fit_transform(
    df_sample[['latitude', 'longitude']]
)

# Apply HDBSCAN (geospatial clustering)

from sklearn.cluster import HDBSCAN

hdb = hdbscan.HDBSCAN(
    min_cluster_size=80,
    min_samples=30,
)

df_sample['cluster'] = hdb.fit_predict(coords_scaled)

print(df_sample['cluster'].value_counts().head())

# Remove noise points
clustered_df = df_sample[df_sample['cluster'] != -1]

# Crime type distribution inside each cluster

print("Clusters found without noise:", clustered_df['cluster'].nunique())
cluster_crime_composition = (
    clustered_df
    .groupby(['cluster', 'crime type'])
    .size()
    .reset_index(name='count')
)

# Percentage composition per cluster
# Calculating the total count for each cluster and then computing the percentage
cluster_crime_composition['percentage'] = (
    cluster_crime_composition['count'] /
    cluster_crime_composition.groupby('cluster')['count'].transform('sum')
) * 100

cluster_crime_composition.head()

# Pivot table (for interpretation & reporting)

pivot_table = cluster_crime_composition.pivot_table(
    index='cluster',
    columns='crime type',
    values='percentage',
    fill_value=0
)

pivot_table.head()

# Vizualization
import matplotlib.pyplot as plt

pivot_table.iloc[1:10].plot(
    kind='bar',
    stacked=True,
    figsize=(12,6)
)

plt.title("Crime Type Composition per Cluster (Top 9 Clusters)")
plt.ylabel("Percentage (%)")
plt.xlabel("Count of Cluster")
plt.legend(bbox_to_anchor=(1.05, 1))
plt.tight_layout()
plt.show()

"""***Vizualization on Cluster-level Temporal Analysis***"""

import pandas as pd

# Load cleaned dataset
df = pd.read_csv("/content/Cleaned_crime_data.csv")

# Keep relevant columns
df = df[['latitude', 'longitude', 'month']].dropna()

# Convert month to datetime
df['month'] = pd.to_datetime(df['month'])

# Random sample
df_sample = df.sample(50000, random_state=42)

print("Sample shape:", df_sample.shape)

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import HDBSCAN
import hdbscan

# Scale geospatial features

scaler = StandardScaler()
coords_scaled = scaler.fit_transform(
    df_sample[['latitude', 'longitude']]
)
# Applying HDBSCAN
hdb = hdbscan.HDBSCAN(
    min_cluster_size=80,
    min_samples=30,
)

df_sample['cluster'] = hdb.fit_predict(coords_scaled)

# Removing noise
clustered_df = df_sample[df_sample['cluster'] != -1]

print("Clusters found:", clustered_df['cluster'].nunique())

# Cluster × Month aggregation
cluster_monthly = (
    clustered_df
    .groupby(['cluster', pd.Grouper(key='month', freq='MS')])
    .size()
    .reset_index(name='crime_count')
)

cluster_monthly.head()

# saving the results
cluster_monthly.to_csv("/content/Cluster_monthly.csv")

# Vizualization of Cluster-level Temporal Analysis
import matplotlib.pyplot as plt

# Select top largest clusters
top_clusters = (
    clustered_df['cluster']
    .value_counts()
    .head(9)
    .index
)

plt.figure(figsize=(12,6))

for c in top_clusters:
    temp = cluster_monthly[cluster_monthly['cluster'] == c]
    plt.plot(temp['month'], temp['crime_count'], label=f'Cluster {c}')

plt.title("Monthly Crime Trends in Major Hotspots")
plt.xlabel("Month")
plt.ylabel("Number of Crimes")
plt.legend()
plt.grid(True)
plt.show()

"""***Vizualization of Crime type composition changes over time within cluster(hotspot evolution)***"""

# Load data & sample
import pandas as pd
import matplotlib.pyplot as plt

# Load cleaned data
df = pd.read_csv("/content/Cleaned_crime_data.csv")

# Keep required columns
df = df[['latitude', 'longitude', 'month', 'crime type']].dropna()

# Convert month to datetime
df['month'] = pd.to_datetime(df['month'])

# Sample
df_sample = df.sample(50000, random_state=42)

# ---- Apply HDBSCAN again ----
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import HDBSCAN

scaler = StandardScaler()
coords_scaled = scaler.fit_transform(df_sample[['latitude', 'longitude']])

hdb = hdbscan.HDBSCAN(
    min_cluster_size=80,
    min_samples=30,
)
df_sample['cluster'] = hdb.fit_predict(coords_scaled)

# Remove noise
clustered_df = df_sample[df_sample['cluster'] != -1]

# Monthly crime count per cluster
cluster_monthly = (
    clustered_df
    .groupby(['cluster', pd.Grouper(key='month', freq='MS')])
    .size()
    .reset_index(name='crime_count')
)

cluster_monthly.head()

# Vizualization of Crime Temporal Analysis within Hotspots (top 5)
top_clusters = (
    clustered_df['cluster']
    .value_counts()
    .head(5)
    .index
)

plt.figure(figsize=(12,6))

for c in top_clusters:
    subset = cluster_monthly[cluster_monthly['cluster'] == c]
    plt.plot(subset['month'], subset['crime_count'], label=f'Cluster {c}')

plt.title("Monthly Crime Trends within Major Hotspots")
plt.xlabel("Month")
plt.ylabel("Number of Crimes")
plt.legend()
plt.show()

"""***Highest Cluster(83) Crime type monthly distribution***"""

cluster_crime_time = (
    clustered_df
    .groupby([
        'cluster',
        pd.Grouper(key='month', freq='MS'),
        'crime type'
    ])
    .size()
    .reset_index(name='count')
)

cluster_crime_time.head()

# Convert to percentages

cluster_crime_time['percentage'] = (
    cluster_crime_time['count'] /
    cluster_crime_time.groupby(['cluster', 'month'])['count'].transform('sum')
) * 100

# Visualise crime-type changes for ONE cluster
cluster_id = top_clusters[0]  # largest hotspot

subset = cluster_crime_time[cluster_crime_time['cluster'] == cluster_id]

pivot = subset.pivot_table(
    index='month',
    columns='crime type',
    values='percentage',
    fill_value=0
)

pivot.plot(
    kind='area',
    stacked=True,
    figsize=(12,6)
)

plt.title(f"Crime Type % Composition Over Time (Cluster {cluster_id})")
plt.ylabel("Percentage (%)")
plt.xlabel("Month")
plt.legend(bbox_to_anchor=(1.05, 1))
plt.tight_layout()
plt.show()

# Crime type Evolution in Top cluster
import pandas as pd
import matplotlib.pyplot as plt


# Therefore, we can directly use 'clustered_df' for aggregation.
crime_trends = (
    clustered_df.groupby(['cluster', 'month', 'crime type'])
      .size()
      .reset_index(name='count')
)

crime_trends.head()
plt.show()
#  major cluster
# Used 'clustered_df' as it contains the 'cluster' column
cluster_id = clustered_df['cluster'].value_counts().idxmax()

cluster_data = crime_trends[crime_trends['cluster'] == cluster_id]

pivot = cluster_data.pivot_table(
    index='month',
    columns='crime type',
    values='count',
    fill_value=0
)

pivot.plot(figsize=(12,6))
plt.title(f"Crime Type Evolution in Cluster {cluster_id}")
plt.xlabel("Month")
plt.ylabel("Number of Crimes")
plt.legend(bbox_to_anchor=(1.05,1))
plt.tight_layout()
plt.show()

"""***Identifing Emerging vs Disappearing hotspots***"""

import pandas as pd
import matplotlib.pyplot as plt

# Load cleaned data WITH cluster labels
df = pd.read_csv("/content/Cleaned_crime_data.csv")

# ensure datetime
df['month'] = pd.to_datetime(df['month'], errors='coerce')

# keep valid geo rows
df = df.dropna(subset=['latitude', 'longitude', 'month'])

# sample data
df_sample = df.sample(50000, random_state=42)

# Scale coordinates
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
coords_scaled = scaler.fit_transform(df_sample[['latitude', 'longitude']])

# HDBSCAN clustering

import hdbscan

clusterer = hdbscan.HDBSCAN(
    min_cluster_size=80,
    min_samples=30,
)

df_sample['cluster'] = clusterer.fit_predict(coords_scaled)

print(df_sample['cluster'].value_counts().head())

# Removing noise
df_hotspots = df_sample[df_sample['cluster'] != -1].copy()

print("Hotspot points:", df_hotspots.shape)
print("Number of clusters:", df_hotspots['cluster'].nunique())

# Monthly crime count per cluster
df_hotspots['month_period'] = df_hotspots['month'].dt.to_period('M')

cluster_month= (
    df_hotspots
    .groupby(['cluster', 'month_period'])
    .size()
    .reset_index(name='crime_count')
)

cluster_month.head()

# Pivot to cluster × month matrix
pivot = cluster_month.pivot_table(
    index='cluster',
    columns='month_period',
    values='crime_count',
    fill_value=0
)

pivot.head()

# Calculate trend
pivot['trend'] = pivot.iloc[:, -1] - pivot.iloc[:, 0]

print(pivot[['trend']].sort_values(by='trend', ascending=False).head())

#  CLASSIFY HOTSPOTS

pivot['hotspot_type'] = 'Stable'
pivot.loc[pivot['trend'] > 10, 'hotspot_type'] = 'Emerging'
pivot.loc[pivot['trend'] < -10, 'hotspot_type'] = 'Disappearing'

pivot['hotspot_type'].value_counts()


# Merging back
df_sample = df_sample.merge(
    pivot[['hotspot_type']],
    left_on='cluster',
    right_index=True,
    how='left'
)
# Count number of clusters per hotspot type
hotspot_counts = pivot['hotspot_type'].value_counts()

print("Hotspot type counts:")
print(hotspot_counts)
print("Total clusters:", hotspot_counts.sum())

# Plot bar chart
plt.figure(figsize=(6,5))
hotspot_counts.plot(kind='bar')

plt.title("Emerging vs Disappearing vs Stable Hotspots (Cluster Count)")
plt.ylabel("Number of Clusters")
plt.xlabel("Hotspot Type")
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

"""***Anlysis on Outcome V/S Hotspots***"""

#Load cleaned dataset & sample

import pandas as pd
import numpy as np

df = pd.read_csv("/content/Cleaned_crime_data.csv")

# Keep required columns only
df = df[['latitude', 'longitude', 'outcome_type', 'month']].dropna(subset=['latitude', 'longitude'])

# 50k random sample
df_sample = df.sample(50000, random_state=42)

print("Sample shape:", df_sample.shape)
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import HDBSCAN
import hdbscan

scaler = StandardScaler()
coords_scaled = scaler.fit_transform(df_sample[['latitude', 'longitude']])

# hotspot detection
hdb = hdbscan.HDBSCAN(
    min_cluster_size=80,
    min_samples=30
)

df_sample['cluster'] = hdb.fit_predict(coords_scaled)

# Convert to binary hotspot label
df_sample['is_cluster'] = df_sample['cluster'].apply(lambda x: 'Hotspot' if x != -1 else 'Non-Hotspot')

df_sample['is_cluster'].value_counts()

# Outcome distribution by hotspot status
outcome_hotspot = (
    df_sample
    .groupby(['is_cluster', 'outcome_type'])
    .size()
    .reset_index(name='count')
)

# Convert to percentage
outcome_hotspot['percentage'] = (
    outcome_hotspot['count'] /
    outcome_hotspot.groupby('is_cluster')['count'].transform('sum')
) * 100

outcome_hotspot.head()
# Visualisation – stacked bar chart
import matplotlib.pyplot as plt

pivot_outcome = outcome_hotspot.pivot_table(
    index='is_cluster',
    columns='outcome_type',
    values='percentage',
    fill_value=0
)

pivot_outcome.plot(
    kind='bar',
    stacked=True,
    figsize=(12,6)
)

plt.title("Outcome Distribution: Hotspot vs Non-Hotspot Crimes")
plt.ylabel("Percentage (%)")
plt.xlabel("Crime Location Type")
plt.legend(bbox_to_anchor=(1.05, 1))
plt.tight_layout()
plt.show()
import seaborn as sns

plt.figure(figsize=(10,6))
sns.heatmap(
    pivot_outcome,
    annot=True,
    fmt=".1f",
    cmap="Reds"
)

plt.title("Heatmap: Outcome Distribution by Hotspot Status")
plt.ylabel("Location Type")
plt.xlabel("Outcome Type")
plt.show()

"""***Vizualisation of Outcome quality inside persistent Hotspots***"""

#  CLASSIFY HOTSPOTS

pivot['hotspot_type'] = 'Stable'
pivot.loc[pivot['trend'] > 10, 'hotspot_type'] = 'Emerging'
pivot.loc[pivot['trend'] < -10, 'hotspot_type'] = 'Disappearing'

pivot['hotspot_type'].value_counts()

# Merge hotspot type back
df_sample = df_sample.merge(
    pivot[['hotspot_type']],
    left_on='cluster',
    right_index=True,
    how='left'
)

# Outcome distribution by hotspot type
outcome_hotspot = (
    df_sample[df_sample['cluster'] != -1]
    .groupby(['hotspot_type', 'outcome_type'])
    .size()
    .reset_index(name='count')
)

# Percentage
outcome_hotspot['percentage'] = (
    outcome_hotspot['count'] /
    outcome_hotspot.groupby('hotspot_type')['count'].transform('sum')
) * 100

outcome_hotspot.head()

#  VISUALISE OUTCOME QUALITY
pivot_outcome = outcome_hotspot.pivot(
    index='hotspot_type',
    columns='outcome_type',
    values='percentage'
).fillna(0)

pivot_outcome.plot(
    kind='bar',
    stacked=True,
    figsize=(12,6)
)

plt.title("Outcome Quality by Hotspot Type")
plt.ylabel("Percentage (%)")
plt.xlabel("Hotspot Category")
plt.legend(bbox_to_anchor=(1.05, 1))
plt.tight_layout()
plt.show()

"""***FINAL GEOSPATIAL MAP (Hotspots Type)***"""

import folium

m = folium.Map(location=[52.5, -1.5], zoom_start=6)

colors = {
    'Emerging': 'red',
    'Disappearing': 'green',
    'Stable': 'blue'
}

for _, row in df_sample[df_sample['cluster'] != -1].iterrows():
    folium.CircleMarker(
        location=[row['latitude'], row['longitude']],
        radius=2,
        color=colors.get(row['hotspot_type'], 'gray'),
        fill=True,
        fill_opacity=0.6
    ).add_to(m)
m.save("hotspot_outcome_map.html")

"""***Preperation of Datasets for Tableau Analysis: Keep only hotspot points (exclude noise)***"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import hdbscan

# Load full cleaned dataset
crime_df = pd.read_csv("/content/Cleaned_crime_data.csv")

crime_df = crime_df[
    [
        'crime_id',
        'month',
        'reported by',
        'falls within',
        'latitude',
        'longitude',
        'location',
        'lsoa name',
        'lsoa code',
        'crime type',
        'last outcome category',
        'outcome_type'
    ]
].dropna(subset=['latitude', 'longitude'])
crime_df['month'] = pd.to_datetime(crime_df['month'])
# Random sample
crime_sample = crime_df.sample(
    n=50000,
    random_state=42
).reset_index(drop=True)

coords = crime_sample[['latitude', 'longitude']]

scaler = StandardScaler()
coords_scaled = scaler.fit_transform(coords)
clusterer = hdbscan.HDBSCAN(
    min_cluster_size=80,
    min_samples=30,
)

crime_sample['cluster'] = clusterer.fit_predict(coords_scaled)
hotspots_df = crime_sample[crime_sample['cluster'] != -1].copy()

print("Hotspot points:", hotspots_df.shape)
print("Number of clusters:", hotspots_df['cluster'].nunique())

# Create monthly period
hotspots_df['month_period'] = hotspots_df['month'].dt.to_period('M')

# Crime count per cluster per month
cluster_month = (
    hotspots_df
    .groupby(['cluster', 'month_period'])
    .size()
    .reset_index(name='crime_count')
)
pivot = cluster_month.pivot_table(
    index='cluster',
    columns='month_period',
    values='crime_count',
    fill_value=0
)

# Trend = last month minus first month
pivot['trend'] = pivot.iloc[:, -1] - pivot.iloc[:, 0]

# Default classification
pivot['hotspot_type'] = 'Stable'
pivot.loc[pivot['trend'] > 10, 'hotspot_type'] = 'Emerging'
pivot.loc[pivot['trend'] < -10, 'hotspot_type'] = 'Disappearing'
hotspot_type_df = pivot[['hotspot_type']].reset_index()

hotspots_df = hotspots_df.merge(
    hotspot_type_df,
    on='cluster',
    how='left'
)


final_df = hotspots_df[
    [
        'crime_id',
        'month',
        'reported by',
        'falls within',
        'latitude',
        'longitude',
        'location',
        'lsoa name',
        'lsoa code',
        'crime type',
        'last outcome category',
        'outcome_type',
        'cluster',
        'hotspot_type'
    ]
]

final_df.to_csv(
    "HDBSCAN_hotspots_points_for_Tableau.csv",
    index=False
)

print("HDBSCAN_hotspots_points_for_tableau.csv created successfully")

# ----------------------------------------------------
# Compute centroid (mean lat/long) & crime count
# ----------------------------------------------------
centroids_lsao = (
    df_hotspots
    .groupby('cluster')
    .agg(
        crime_count=('latitude', 'count'),
        centroid_latitude=('latitude', 'mean'),
        centroid_longitude=('longitude', 'mean')
    )
    .reset_index()
)

# ----------------------------------------------------
#  Get most frequent LSOA per cluster (mode)
# ----------------------------------------------------
lsoa_info = (
    df_hotspots
    .groupby('cluster')
    .agg(
        lsoa_name=('lsoa name', lambda x: x.mode().iloc[0] if not x.mode().empty else 'Unknown'),
        lsoa_code=('lsoa code', lambda x: x.mode().iloc[0] if not x.mode().empty else 'Unknown')
    )
    .reset_index()
)

# ----------------------------------------------------
# 3. Join centroids with LSOA info
# ----------------------------------------------------
top_areas = centroids_lsao.merge(lsoa_info, on='cluster', how='left')

# ----------------------------------------------------
# 4. Sort by crime count (high risk first)
# ----------------------------------------------------
top_areas = top_areas.sort_values(by='crime_count', ascending=False)

print(top_areas.head(10))

# ----------------------------------------------------
# 5. Export for Tableau
# ----------------------------------------------------
output_path = "/content/top_high_risk_areas_with_Centroids.csv"
top_areas.to_csv(output_path, index=False)

print(f"Exported to: {output_path}")

"""***Verifying the data for Tableau Analysis***"""

final_df.head()